{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e473eaad-50d7-4400-910a-41ee2a2c3ac3",
   "metadata": {},
   "source": [
    "# Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fea6bba-7513-4383-adcf-df2cd3ba3018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyathena in /opt/conda/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /opt/conda/lib/python3.10/site-packages (from pyathena) (1.33.9)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /opt/conda/lib/python3.10/site-packages (from pyathena) (1.33.9)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from pyathena) (8.0.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from pyathena) (2022.7.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.26.4->pyathena) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.26.4->pyathena) (0.8.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.29.4->pyathena) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.29.4->pyathena) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.4->pyathena) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.207.1)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.33.9)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.25.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.1.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.20.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.5.2)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.0.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.64.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.0)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.9 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.33.9)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from docker->sagemaker) (0.58.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->sagemaker) (2023.11.17)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.7 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.15)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyathena\n",
    "!pip3 install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d48bbf4-d38f-423a-b014-bf2599a7de15",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56617f56-6a2c-4d60-9b60-8d75ab10d4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3 # aws sdk for python\n",
    "import sagemaker # machine learning platform\n",
    "import numpy as np # array manipulation\n",
    "import os # operating system interfaces\n",
    "import pandas as pd # python data analysis\n",
    "import re # regular expressions\n",
    "from pyathena import connect # athena client\n",
    "from sagemaker.pytorch.estimator import PyTorch # PyTorch estimator\n",
    "from time import gmtime, strftime, sleep # time-related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e50ffb-1208-4663-bba0-a203890c410a",
   "metadata": {},
   "source": [
    "# Perform Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7735c026-0094-4bde-a03e-50336d86837a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory location: s3://sagemaker-us-east-1-752648173624/safety/data\n",
      "FeatureStore directory location: s3://sagemaker-us-east-1-752648173624/safety-featurestore\n",
      "\n",
      "Execution Role: arn:aws:iam::752648173624:role/LabRole\n"
     ]
    }
   ],
   "source": [
    "# establish sagemaker session, provide permissions\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# create a boto3 session for the sagemaker service\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "# client to make featurestore record calls\n",
    "featurestore_runtime = boto3.Session().client(\n",
    "    service_name=\"sagemaker-featurestore-runtime\", region_name=region\n",
    ")\n",
    "\n",
    "# create boto3 session to establish feature store session\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "# create featurestore session\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sm,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime,\n",
    ")\n",
    "\n",
    "# define prefixes for the safety data directory and featurestore\n",
    "prefix_data = 'safety/data'\n",
    "prefix_featurestore = 'safety-featurestore'\n",
    "\n",
    "#-------------------------------------------\n",
    "s3_capture_upload_path = f\"s3://{bucket}/{prefix_data}\"\n",
    "s3_report_path = f\"s3://{bucket}/{prefix_featurestore}\\n\"\n",
    "#-------------------------------------------\n",
    "\n",
    "# print s3 locations\n",
    "print('Data directory location:', f\"s3://{bucket}/{prefix_data}\")\n",
    "print('FeatureStore directory location:', f\"s3://{bucket}/{prefix_featurestore}\\n\")\n",
    "\n",
    "# print current IAM role for notebook instance\n",
    "role = sagemaker.get_execution_role()\n",
    "print('Execution Role:', role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ade40-dec1-4ea6-96fd-7fc4c2835c6e",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5549b1-2b6b-4b81-8162-a3c1b97ee351",
   "metadata": {},
   "source": [
    "## Query Catalog Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad88c943-d9f4-41ab-bcab-95e8783d92d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL query SELECT statement:\n",
      " SELECT * FROM safetydb.catalog_csv\n",
      "    WHERE img_filename like '%.jpg'\n",
      "    AND label_filename like '%.txt'\n",
      "    LIMIT 25000\n"
     ]
    }
   ],
   "source": [
    "# define database name\n",
    "database_name = 'safetydb'\n",
    "\n",
    "# define table name\n",
    "table_name_csv = 'catalog_csv'\n",
    "\n",
    "# set s3 temporary staging directory\n",
    "s3_staging_dir = \"s3://{0}/athena/staging\".format(bucket)\n",
    "\n",
    "# define connection parameters\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)\n",
    "\n",
    "# define sql query statement\n",
    "statement = \"\"\"SELECT * FROM {}.{}\n",
    "    WHERE img_filename like '%.jpg'\n",
    "    AND label_filename like '%.txt'\n",
    "    LIMIT 25000\"\"\".format(\n",
    "    database_name, table_name_csv\n",
    ")\n",
    "\n",
    "# print sql statement for review before executing\n",
    "print('SQL query SELECT statement:\\n', statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e47b28e4-95d6-4be9-bb4d-fff2cb4e5d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1293/3145676702.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_catalog_query = pd.read_sql(statement, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>img_filename</th>\n",
       "      <th>label_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>000001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>000002.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003</td>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>000003.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004</td>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>000004.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005</td>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>000005.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000006</td>\n",
       "      <td>000006.jpg</td>\n",
       "      <td>000006.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000007</td>\n",
       "      <td>000007.jpg</td>\n",
       "      <td>000007.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000008</td>\n",
       "      <td>000008.jpg</td>\n",
       "      <td>000008.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000009</td>\n",
       "      <td>000009.jpg</td>\n",
       "      <td>000009.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000010</td>\n",
       "      <td>000010.jpg</td>\n",
       "      <td>000010.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id img_filename label_filename\n",
       "0    000001   000001.jpg     000001.txt\n",
       "1    000002   000002.jpg     000002.txt\n",
       "2    000003   000003.jpg     000003.txt\n",
       "3    000004   000004.jpg     000004.txt\n",
       "4    000005   000005.jpg     000005.txt\n",
       "5    000006   000006.jpg     000006.txt\n",
       "6    000007   000007.jpg     000007.txt\n",
       "7    000008   000008.jpg     000008.txt\n",
       "8    000009   000009.jpg     000009.txt\n",
       "9    000010   000010.jpg     000010.txt"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute sql query and display results\n",
    "df_catalog_query = pd.read_sql(statement, conn)\n",
    "df_catalog_query.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c587ff-0c7a-42ec-b6b0-0d72d06af5cb",
   "metadata": {},
   "source": [
    "## Combine with FeatureGroup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478284af-660f-44ff-94af-ba2d21b854e8",
   "metadata": {},
   "source": [
    "### Get Batch Records from FeatureGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cf22ef3-75cd-4da7-a5cc-888d4b4bd04a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image FeatureGroup Name: image-feature-group-07-13-08-39\n",
      "Label FeatureGroup Name: label-feature-group-07-13-08-39\n"
     ]
    }
   ],
   "source": [
    "# define featuregroup name patterns\n",
    "image_feature_group_name_pattern = 'image-feature-group-'\n",
    "label_feature_group_name_pattern = 'label-feature-group-'\n",
    "\n",
    "# obtain latest version of featuregroup names, if multiples exist\n",
    "all_image_feature_groups = sm.list_feature_groups(NameContains=image_feature_group_name_pattern, SortBy='CreationTime', SortOrder='Descending')\n",
    "all_label_feature_groups = sm.list_feature_groups(NameContains=label_feature_group_name_pattern, SortBy='CreationTime', SortOrder='Descending')\n",
    "image_feature_group_name = all_image_feature_groups['FeatureGroupSummaries'][0]['FeatureGroupName']\n",
    "label_feature_group_name = all_label_feature_groups['FeatureGroupSummaries'][0]['FeatureGroupName']\n",
    "\n",
    "# print featuregroup names\n",
    "print('Image FeatureGroup Name:', image_feature_group_name)\n",
    "print('Label FeatureGroup Name:', label_feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "685da3b7-c927-4b31-9040-6c89142c754c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify record identifiers (i.e., sample ids) from previous catalog query\n",
    "record_identifiers_value = df_catalog_query['sample_id'].values.astype(str).tolist()\n",
    "\n",
    "# query image featuregroup by using record_id as primary key\n",
    "df_image_feature_group = featurestore_runtime.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\n",
    "            'FeatureGroupName': image_feature_group_name,\n",
    "            'RecordIdentifiersValueAsString':record_identifiers_value,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# query label featuregroup by using record_id as primary key\n",
    "df_label_feature_group = featurestore_runtime.batch_get_record(\n",
    "    Identifiers=[\n",
    "        {\n",
    "            'FeatureGroupName': label_feature_group_name,\n",
    "            'RecordIdentifiersValueAsString': record_identifiers_value,\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c9c87-50c0-4428-adcf-7ec7c769cfee",
   "metadata": {},
   "source": [
    "### Display Image FeatureGroup Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ae6fb9e-fb30-4ddd-b3fc-809fa2ed163e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>img_format</th>\n",
       "      <th>img_mode</th>\n",
       "      <th>img_height</th>\n",
       "      <th>img_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000012</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000006</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000016</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000011</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id img_format img_mode img_height img_width\n",
       "0    000012       JPEG      RGB        640       640\n",
       "1    000006       JPEG      RGB        640       640\n",
       "2    000016       JPEG      RGB        640       640\n",
       "3    000002       JPEG      RGB        640       640\n",
       "4    000011       JPEG      RGB        640       640"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of image records and feature names, exclude eventtime\n",
    "image_records = [sample['Record'] for sample in df_image_feature_group['Records']]\n",
    "image_feature_names = [feature['FeatureName'] for feature in image_records[0] if feature['FeatureName'] != 'EventTime']\n",
    "\n",
    "# create list of image data\n",
    "image_data = [image_feature_names]\n",
    "\n",
    "# iterate through each record in image featuregroup\n",
    "for record in image_records:\n",
    "    \n",
    "    # iterate through each feature in individual record\n",
    "    image_data.append([feature['ValueAsString'] for feature in record if feature['FeatureName'] != 'EventTime'])\n",
    "                       \n",
    "# create images dataframe\n",
    "df_images = pd.DataFrame(image_data[1:], columns=image_data[0])\n",
    "                       \n",
    "# display dataframe head\n",
    "df_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0184067b-e7c1-45aa-8874-3b03c04c17fa",
   "metadata": {},
   "source": [
    "### Display Label FeatureGroup Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8ee1536-0dc1-45ab-8a2a-10c88a9840d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>count_helmet</th>\n",
       "      <th>count_vest</th>\n",
       "      <th>count_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000012</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000011</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id count_helmet count_vest count_head\n",
       "0    000012            4          0          0\n",
       "1    000006            0          0          5\n",
       "2    000016            1          0          0\n",
       "3    000002            2          1          0\n",
       "4    000011           13          0          0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of label records and feature names, exclude eventtime\n",
    "label_records = [sample['Record'] for sample in df_label_feature_group['Records']]\n",
    "label_feature_names = [feature['FeatureName'] for feature in label_records[0] if feature['FeatureName'] != 'EventTime']\n",
    "\n",
    "# create list of label data\n",
    "label_data = [label_feature_names]\n",
    "\n",
    "# iterate through each record in label featuregroup\n",
    "for record in label_records:\n",
    "    \n",
    "    # iterate through each feature in individual record\n",
    "    label_data.append([feature['ValueAsString'] for feature in record if feature['FeatureName'] != 'EventTime'])\n",
    "                       \n",
    "# create labels dataframe\n",
    "df_labels = pd.DataFrame(label_data[1:], columns=label_data[0])\n",
    "                       \n",
    "# display dataframe head\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd85fc-f5cd-45f9-acbc-679479d822cb",
   "metadata": {},
   "source": [
    "### Join Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3731579c-abd3-4587-b200-1968c641fe73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>img_filename</th>\n",
       "      <th>label_filename</th>\n",
       "      <th>img_format</th>\n",
       "      <th>img_mode</th>\n",
       "      <th>img_height</th>\n",
       "      <th>img_width</th>\n",
       "      <th>count_helmet</th>\n",
       "      <th>count_vest</th>\n",
       "      <th>count_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>000001.txt</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>000002.txt</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003</td>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>000003.txt</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004</td>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>000004.txt</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005</td>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>000005.txt</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>RGB</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id img_filename label_filename img_format img_mode img_height  \\\n",
       "0    000001   000001.jpg     000001.txt       JPEG      RGB        640   \n",
       "1    000002   000002.jpg     000002.txt       JPEG      RGB        640   \n",
       "2    000003   000003.jpg     000003.txt       JPEG      RGB        640   \n",
       "3    000004   000004.jpg     000004.txt       JPEG      RGB        640   \n",
       "4    000005   000005.jpg     000005.txt       JPEG      RGB        640   \n",
       "\n",
       "  img_width count_helmet count_vest count_head  \n",
       "0       640            2          0          0  \n",
       "1       640            2          1          0  \n",
       "2       640            4          0          0  \n",
       "3       640            0          0          5  \n",
       "4       640            2          0          0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge catalog query and images dataframes, then the resulting with labels dataframe\n",
    "df_combined = pd.merge(df_catalog_query, df_images, on='sample_id')\n",
    "df_combined = pd.merge(df_combined, df_labels, on='sample_id')\n",
    "\n",
    "# display resulting datafrmae\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6845240-c8a3-45a3-a639-e0d87def0700",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "734ba1fc-755d-4d31-b236-bc50d6c60a60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Splits:\n",
      "------------\n",
      "Train :   9 samples\n",
      "Val   :   3 samples\n",
      "Test  :   0 samples\n",
      "Batch :   8 samples\n"
     ]
    }
   ],
   "source": [
    "# use a copy of dataframe, can manipulate if desired for experimentation\n",
    "data = df_combined.copy()\n",
    "\n",
    "# data split in four sets - training, validation, test, and batch inference\n",
    "rand_split = np.random.rand(len(data))\n",
    "train_list = rand_split < 0.4\n",
    "val_list = (rand_split >= 0.4) & (rand_split < 0.5)\n",
    "test_list = (rand_split >= 0.5) & (rand_split < 0.6)\n",
    "batch_list = rand_split >= 0.6 # \"production\" data\n",
    "\n",
    "# print data splits\n",
    "print('Data Splits:')\n",
    "print('------------')\n",
    "print(f\"Train :   {sum(train_list)} samples\")\n",
    "print(f\"Val   :   {sum(val_list)} samples\")\n",
    "print(f\"Test  :   {sum(test_list)} samples\")\n",
    "print(f\"Batch :   {sum(batch_list)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b0b53-c0f5-48db-a095-d0ee8f2d175d",
   "metadata": {},
   "source": [
    "## Create Split Datasets in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d6c1b29-3977-49d1-9c56-a8ab90267061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images source directory location: s3://sagemaker-us-east-1-752648173624/safety/data/images/\n",
      "Labels source directory location: s3://sagemaker-us-east-1-752648173624/safety/data/labels/ \n",
      "\n",
      "Split destination directory location: s3://sagemaker-us-east-1-752648173624/safety/data/split/\n"
     ]
    }
   ],
   "source": [
    "# define and print source s3 locations\n",
    "s3_images_source = f\"s3://{bucket}/{prefix_data}/images/\"\n",
    "s3_labels_source = f\"s3://{bucket}/{prefix_data}/labels/\"\n",
    "print('Images source directory location:', s3_images_source)\n",
    "print('Labels source directory location:', s3_labels_source, '\\n')\n",
    "\n",
    "# define and print destination s3 location for data splits\n",
    "s3_split_dest = f\"s3://{bucket}/{prefix_data}/split/\"\n",
    "print('Split destination directory location:', s3_split_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07b7727e-d8cc-4e70-89e4-79e0970740d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define function to copy files for respective data splits to corresponding s3 destinations\n",
    "# provide 'split_name' as either 'train', 'val', 'test', or 'batch'\n",
    "# provide 'split_list' as either 'train_list', 'val_list', 'test_list', or 'batch_list'\n",
    "def split_dataset(split_name, split_list):\n",
    "\n",
    "    # iterate through each sample in split\n",
    "    for index, sample in data[split_list].iterrows():\n",
    "\n",
    "        # source/destination variables for individual sample\n",
    "        cp_image_source = f\"{s3_images_source}{sample['img_filename']}\"\n",
    "        cp_image_dest = f\"{s3_split_dest}{split_name}/images/\"\n",
    "        cp_label_source = f\"{s3_labels_source}{sample['label_filename']}\"\n",
    "        cp_label_dest = f\"{s3_split_dest}{split_name}/labels/\"\n",
    "\n",
    "        # copy from source to destination\n",
    "        !aws s3 cp $cp_image_source $cp_image_dest\n",
    "        !aws s3 cp $cp_label_source $cp_label_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27d9bd4b-5eee-42d5-b1b8-cf402f5e516f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning TRAIN data split copies.\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000002.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000002.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000002.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000002.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000003.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000003.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000003.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000003.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000004.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000004.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000004.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000004.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000005.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000005.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000005.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000005.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000007.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000007.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000007.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000007.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000014.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000014.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000014.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000014.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000017.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000017.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000017.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000017.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000018.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000018.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000018.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000018.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000019.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/images/000019.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000019.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/train/labels/000019.txt\n",
      "Completed TRAIN data split copies.\n",
      "\n",
      "Beginning VAL data split copies.\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000010.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/val/images/000010.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000010.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/val/labels/000010.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000011.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/val/images/000011.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000011.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/val/labels/000011.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000016.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/val/images/000016.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000016.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/val/labels/000016.txt\n",
      "Completed VAL data split copies.\n",
      "\n",
      "Beginning TEST data split copies.\n",
      "Completed TEST data split copies.\n",
      "\n",
      "Beginning BATCH data split copies.\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000001.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000001.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000001.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000006.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000006.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000006.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000006.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000008.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000008.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000008.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000008.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000009.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000009.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000009.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000009.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000012.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000012.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000012.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000012.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000013.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000013.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000013.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000013.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000015.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000015.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000015.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000015.txt\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/images/000020.jpg to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000020.jpg\n",
      "copy: s3://sagemaker-us-east-1-752648173624/safety/data/labels/000020.txt to s3://sagemaker-us-east-1-752648173624/safety/data/split/batch/labels/000020.txt\n",
      "Completed BATCH data split copies.\n"
     ]
    }
   ],
   "source": [
    "# perform data copies\n",
    "print('Beginning TRAIN data split copies.')\n",
    "split_dataset(split_name='train', split_list=train_list)\n",
    "print('Completed TRAIN data split copies.\\n')\n",
    "\n",
    "print('Beginning VAL data split copies.')\n",
    "split_dataset(split_name='val', split_list=val_list)\n",
    "print('Completed VAL data split copies.\\n')\n",
    "\n",
    "print('Beginning TEST data split copies.')\n",
    "split_dataset(split_name='test', split_list=test_list)\n",
    "print('Completed TEST data split copies.\\n')\n",
    "\n",
    "print('Beginning BATCH data split copies.')\n",
    "split_dataset(split_name='batch', split_list=batch_list)\n",
    "print('Completed BATCH data split copies.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7dfc0-e357-4eba-84a3-9b455b9d4c60",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training Job and Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f3e67-2d56-49b1-9c9b-1abec7b6d243",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create and Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cc59507-eef2-4a8c-b6e5-f5ab063581af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: yolov8-2024-02-11-19-09-42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-11 19:09:43 Starting - Starting the training job...\n",
      "2024-02-11 19:09:58 Starting - Preparing the instances for training......\n",
      "2024-02-11 19:11:08 Downloading - Downloading input data...\n",
      "2024-02-11 19:11:38 Downloading - Downloading the training image......\n",
      "2024-02-11 19:12:23 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:30,423 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:30,424 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:30,424 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:30,434 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:30,436 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:31,870 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting ultralytics==8.1.9 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading ultralytics-8.1.9-py3-none-any.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.2/40.2 kB 6.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (1.26.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (4.8.1.78)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (10.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (1.11.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from ultralytics==8.1.9->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting thop>=0.1.1 (from ultralytics==8.1.9->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r requirements.txt (line 1)) (0.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (4.45.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.1.9->-r requirements.txt (line 1)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.1.9->-r requirements.txt (line 1)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics==8.1.9->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading ultralytics-8.1.9-py3-none-any.whl (709 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 709.3/709.3 kB 31.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, thop, ultralytics\u001b[0m\n",
      "\u001b[34mSuccessfully installed py-cpuinfo-9.0.0 thop-0.1.1.post2209072238 ultralytics-8.1.9\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,176 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,176 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,177 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,177 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,188 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,189 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,199 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,200 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,210 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"data\": \"data.yaml\",\n",
      "        \"epochs\": 5,\n",
      "        \"saved_model_name\": \"benchmark_model.pt\",\n",
      "        \"yolo_model\": \"yolov8n.pt\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"yolov8-2024-02-11-19-09-42\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-752648173624/yolov8-2024-02-11-19-09-42/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"data\":\"data.yaml\",\"epochs\":5,\"saved_model_name\":\"benchmark_model.pt\",\"yolo_model\":\"yolov8n.pt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-752648173624/yolov8-2024-02-11-19-09-42/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"data\":\"data.yaml\",\"epochs\":5,\"saved_model_name\":\"benchmark_model.pt\",\"yolo_model\":\"yolov8n.pt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"yolov8-2024-02-11-19-09-42\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-752648173624/yolov8-2024-02-11-19-09-42/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--data\",\"data.yaml\",\"--epochs\",\"5\",\"--saved_model_name\",\"benchmark_model.pt\",\"--yolo_model\",\"yolov8n.pt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATA=data.yaml\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_SAVED_MODEL_NAME=benchmark_model.pt\u001b[0m\n",
      "\u001b[34mSM_HP_YOLO_MODEL=yolov8n.pt\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --data data.yaml --epochs 5 --saved_model_name benchmark_model.pt --yolo_model yolov8n.pt\u001b[0m\n",
      "\u001b[34m2024-02-11 19:12:35,236 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mDownloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\u001b[0m\n",
      "\u001b[34m0%|          | 0.00/6.23M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 6.23M/6.23M [00:00<00:00, 344MB/s]\u001b[0m\n",
      "\u001b[34mNew https://pypi.org/project/ultralytics/8.1.11 available 😃 Update with 'pip install -U ultralytics'\u001b[0m\n",
      "\u001b[34mUltralytics YOLOv8.1.9 🚀 Python-3.10.8 torch-2.0.1 CPU (Intel Xeon Platinum 8175M 2.50GHz)\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mengine/trainer: #033[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=5, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\u001b[0m\n",
      "\u001b[34mDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\u001b[0m\n",
      "\u001b[34m0%|          | 0.00/755k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 755k/755k [00:00<00:00, 135MB/s]\u001b[0m\n",
      "\u001b[34mOverriding model.yaml nc=80 with nc=3\u001b[0m\n",
      "\u001b[34mfrom  n    params  module                                       arguments\u001b[0m\n",
      "\u001b[34m0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]\u001b[0m\n",
      "\u001b[34m1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]\u001b[0m\n",
      "\u001b[34m2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]\u001b[0m\n",
      "\u001b[34m3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]\u001b[0m\n",
      "\u001b[34m4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]\u001b[0m\n",
      "\u001b[34m5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]\u001b[0m\n",
      "\u001b[34m6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]\u001b[0m\n",
      "\u001b[34m7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]\u001b[0m\n",
      "\u001b[34m8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]\u001b[0m\n",
      "\u001b[34m9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]\u001b[0m\n",
      "\u001b[34m10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\u001b[0m\n",
      "\u001b[34m11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]\u001b[0m\n",
      "\u001b[34m13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']\u001b[0m\n",
      "\u001b[34m14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]\u001b[0m\n",
      "\u001b[34m16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]\u001b[0m\n",
      "\u001b[34m17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]\u001b[0m\n",
      "\u001b[34m19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]\u001b[0m\n",
      "\u001b[34m20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]\u001b[0m\n",
      "\u001b[34m21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]\u001b[0m\n",
      "\u001b[34m22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]\u001b[0m\n",
      "\u001b[34mModel summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\u001b[0m\n",
      "\u001b[34mTransferred 319/355 items from pretrained weights\u001b[0m\n",
      "\u001b[34mFreezing layer 'model.22.dfl.conv.weight'\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mtrain: #033[0mScanning /opt/ml/input/data/training/train/labels...:   0%|          | 0/18 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mtrain: #033[0mScanning /opt/ml/input/data/training/train/labels... 18 images, 0 backgrounds, 0 corrupt: 100%|██████████| 18/18 [00:00<00:00, 1248.66it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mtrain: #033[0mNew cache created: /opt/ml/input/data/training/train/labels.cache\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mval: #033[0mScanning /opt/ml/input/data/training/val/labels...:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mval: #033[0mScanning /opt/ml/input/data/training/val/labels... 8 images, 0 backgrounds, 0 corrupt: 100%|██████████| 8/8 [00:00<00:00, 1603.71it/s]\u001b[0m\n",
      "\u001b[34m#033[34m#033[1mval: #033[0mNew cache created: /opt/ml/input/data/training/val/labels.cache\u001b[0m\n",
      "\u001b[34mPlotting labels to runs/detect/train/labels.jpg...\u001b[0m\n",
      "\u001b[34m#033[34m#033[1moptimizer:#033[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically...\u001b[0m\n",
      "\u001b[34m#033[34m#033[1moptimizer:#033[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\u001b[0m\n",
      "\u001b[34mImage sizes 640 train, 640 val\u001b[0m\n",
      "\u001b[34mUsing 0 dataloader workers\u001b[0m\n",
      "\u001b[34mLogging results to #033[1mruns/detect/train#033[0m\u001b[0m\n",
      "\u001b[34mStarting training for 5 epochs...\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1/5         0G      1.843      3.931      1.655         83        640:   0%|          | 0/2 [00:05<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1/5         0G      1.843      3.931      1.655         83        640:  50%|█████     | 1/2 [00:05<00:05,  5.45s/it]\u001b[0m\n",
      "\u001b[34m1/5         0G      1.859      4.108      1.563         15        640:  50%|█████     | 1/2 [00:06<00:05,  5.45s/it]\u001b[0m\n",
      "\u001b[34m1/5         0G      1.859      4.108      1.563         15        640: 100%|██████████| 2/2 [00:06<00:00,  2.67s/it]\u001b[0m\n",
      "\u001b[34m1/5         0G      1.859      4.108      1.563         15        640: 100%|██████████| 2/2 [00:06<00:00,  3.09s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.21s/it]\u001b[0m\n",
      "\u001b[34mall          8         33    0.00276      0.177    0.00423    0.00139\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2/5         0G      1.956      4.128      1.605         74        640:   0%|          | 0/2 [00:04<?, ?it/s]\u001b[0m\n",
      "\u001b[34m2/5         0G      1.956      4.128      1.605         74        640:  50%|█████     | 1/2 [00:04<00:04,  4.58s/it]\u001b[0m\n",
      "\u001b[34m2/5         0G      1.739      3.895      1.575         10        640:  50%|█████     | 1/2 [00:05<00:04,  4.58s/it]\u001b[0m\n",
      "\u001b[34m2/5         0G      1.739      3.895      1.575         10        640: 100%|██████████| 2/2 [00:05<00:00,  2.27s/it]\u001b[0m\n",
      "\u001b[34m2/5         0G      1.739      3.895      1.575         10        640: 100%|██████████| 2/2 [00:05<00:00,  2.62s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\u001b[0m\n",
      "\u001b[34mall          8         33    0.00229     0.0517    0.00389    0.00168\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3/5         0G      2.003      4.064      1.651        121        640:   0%|          | 0/2 [00:04<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3/5         0G      2.003      4.064      1.651        121        640:  50%|█████     | 1/2 [00:04<00:04,  4.67s/it]\u001b[0m\n",
      "\u001b[34m3/5         0G      2.131      4.193      1.804          3        640:  50%|█████     | 1/2 [00:05<00:04,  4.67s/it]\u001b[0m\n",
      "\u001b[34m3/5         0G      2.131      4.193      1.804          3        640: 100%|██████████| 2/2 [00:05<00:00,  2.40s/it]\u001b[0m\n",
      "\u001b[34m3/5         0G      2.131      4.193      1.804          3        640: 100%|██████████| 2/2 [00:05<00:00,  2.74s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\u001b[0m\n",
      "\u001b[34mall          8         33    0.00358      0.194    0.00506    0.00215\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m4/5         0G      1.735      3.938      1.534         81        640:   0%|          | 0/2 [00:04<?, ?it/s]\u001b[0m\n",
      "\u001b[34m4/5         0G      1.735      3.938      1.534         81        640:  50%|█████     | 1/2 [00:04<00:04,  4.14s/it]\u001b[0m\n",
      "\u001b[34m4/5         0G      1.787       3.78      1.543         22        640:  50%|█████     | 1/2 [00:04<00:04,  4.14s/it]\u001b[0m\n",
      "\u001b[34m4/5         0G      1.787       3.78      1.543         22        640: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]\u001b[0m\n",
      "\u001b[34m4/5         0G      1.787       3.78      1.543         22        640: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\u001b[0m\n",
      "\u001b[34mall          8         33     0.0037      0.194    0.00662    0.00313\u001b[0m\n",
      "\u001b[34mEpoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5/5         0G      1.668      3.952       1.41         57        640:   0%|          | 0/2 [00:04<?, ?it/s]\u001b[0m\n",
      "\u001b[34m5/5         0G      1.668      3.952       1.41         57        640:  50%|█████     | 1/2 [00:04<00:04,  4.09s/it]\u001b[0m\n",
      "\u001b[34m5/5         0G      1.744      4.256      1.507          5        640:  50%|█████     | 1/2 [00:04<00:04,  4.09s/it]\u001b[0m\n",
      "\u001b[34m5/5         0G      1.744      4.256      1.507          5        640: 100%|██████████| 2/2 [00:04<00:00,  2.02s/it]#015        5/5         0G      1.744      4.256      1.507          5        640: 100%|██████████| 2/2 [00:04<00:00,  2.33s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\u001b[0m\n",
      "\u001b[34mall          8         33    0.00421      0.319    0.00822    0.00377\u001b[0m\n",
      "\u001b[34m5 epochs completed in 0.010 hours.\u001b[0m\n",
      "\u001b[34mOptimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\u001b[0m\n",
      "\u001b[34mOptimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\u001b[0m\n",
      "\u001b[34mValidating runs/detect/train/weights/best.pt...\u001b[0m\n",
      "\u001b[34mUltralytics YOLOv8.1.9 🚀 Python-3.10.8 torch-2.0.1 CPU (Intel Xeon Platinum 8175M 2.50GHz)\u001b[0m\n",
      "\u001b[34mModel summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\u001b[0m\n",
      "\u001b[34mClass     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\u001b[0m\n",
      "\u001b[34mall          8         33    0.00422      0.319     0.0083    0.00379\u001b[0m\n",
      "\u001b[34mhelmet          8         29    0.00643      0.138     0.0146    0.00739\u001b[0m\n",
      "\u001b[34mvest          8          4      0.002        0.5    0.00196   0.000196\u001b[0m\n",
      "\u001b[34mSpeed: 0.8ms preprocess, 77.6ms inference, 0.0ms loss, 33.0ms postprocess per image\u001b[0m\n",
      "\u001b[34mResults saved to #033[1mruns/detect/train#033[0m\u001b[0m\n",
      "\u001b[34m2024-02-11 19:13:26,475 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-02-11 19:13:26,476 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-02-11 19:13:26,476 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-02-11 19:13:45 Uploading - Uploading generated training model\n",
      "2024-02-11 19:13:45 Completed - Training job completed\n",
      "Training seconds: 157\n",
      "Billable seconds: 157\n",
      "CPU times: user 509 ms, sys: 60.9 ms, total: 569 ms\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# define job name and output location\n",
    "job_name = 'yolov8-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "output_location = \"s3://{}/{}/output/{}\".format(bucket, prefix_data, job_name)\n",
    "\n",
    "# build a PyTorch estimator\n",
    "pytorch_estimator = PyTorch(\n",
    "    role=role,\n",
    "    entry_point='train.py', # custom training script, locate in code directory\n",
    "    framework_version='2.0.1', # training - CPU - Python 3.10\n",
    "    py_version='py310',\n",
    "    source_dir='./code',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sess,\n",
    "    hyperparameters = {'data': 'data.yaml', # yaml config file for custom dataset\n",
    "                       'epochs': 5, # number of training epochs\n",
    "                       'yolo_model': 'yolov8n.pt', # pretrained base model\n",
    "                       'saved_model_name': 'benchmark_model.pt' # name for model export\n",
    "                      }\n",
    ")\n",
    "\n",
    "# s3 location where training data is saved\n",
    "inputs = s3_split_dest[:-1]\n",
    "\n",
    "# begin training job\n",
    "pytorch_estimator.fit(inputs=inputs, job_name=job_name, logs='All')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ec59e-08ef-4e4d-9cc3-2e74d7898db4",
   "metadata": {},
   "source": [
    "## Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf873d55-c97e-4852-aa0b-726583e26b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transform job..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29451de5-bab2-4580-b100-45dd3490d23f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-752648173624/safety/data/output/yolov8-2024-02-11-19-09-42/yolov8-2024-02-11-19-09-42/output/model.tar.gz), script artifact (code), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-752648173624/pytorch-inference-2024-02-11-19-16-12-669/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2024-02-11-19-16-19-361\n",
      "INFO:sagemaker:Creating transform job with name: pytorch-inference-2024-02-11-19-16-20-093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\u001b[34mCollecting ultralytics==8.1.9 (from -r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading ultralytics-8.1.9-py3-none-any.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.2/40.2 kB 2.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.9.0.80)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (10.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.1.0+cpu)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (0.16.0+cpu)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting thop>=0.1.1 (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.5.1)\u001b[0m\n",
      "\u001b[34mCollecting seaborn>=0.11.0 (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.47.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2022.6)\u001b[0m\n",
      "\u001b[35mCollecting ultralytics==8.1.9 (from -r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading ultralytics-8.1.9-py3-none-any.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.2/40.2 kB 2.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.22.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.9.0.80)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (10.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.10.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.1.0+cpu)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (0.16.0+cpu)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[35mCollecting py-cpuinfo (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mCollecting thop>=0.1.1 (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.5.1)\u001b[0m\n",
      "\u001b[35mCollecting seaborn>=0.11.0 (from ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (0.12.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.47.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.4.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2023.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading ultralytics-8.1.9-py3-none-any.whl (709 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 709.3/709.3 kB 40.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 31.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, thop, seaborn, ultralytics\u001b[0m\n",
      "\u001b[34mSuccessfully installed py-cpuinfo-9.0.0 seaborn-0.13.2 thop-0.1.1.post2209072238 ultralytics-8.1.9\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWarning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,118 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,123 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2023.11.17)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (4.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.2.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2023.12.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics==8.1.9->-r /opt/ml/model/code/requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[35mDownloading ultralytics-8.1.9-py3-none-any.whl (709 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 709.3/709.3 kB 40.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 31.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling collected packages: py-cpuinfo, thop, seaborn, ultralytics\u001b[0m\n",
      "\u001b[35mSuccessfully installed py-cpuinfo-9.0.0 seaborn-0.13.2 thop-0.1.1.post2209072238 ultralytics-8.1.9\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.3.2 -> 24.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mWarning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[35mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,118 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,123 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,196 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,324 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.8.2\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mMetrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3892 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.10\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,196 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,324 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[35mTorchserve version: 0.8.2\u001b[0m\n",
      "\u001b[35mTS Home: /opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mCurrent directory: /\u001b[0m\n",
      "\u001b[35mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[35mMetrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\u001b[0m\n",
      "\u001b[35mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[35mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[35mMax heap size: 3892 M\u001b[0m\n",
      "\u001b[35mPython executable: /opt/conda/bin/python3.10\u001b[0m\n",
      "\u001b[35mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[35mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mMetrics mode: log\u001b[0m\n",
      "\u001b[34mDisable system metrics: false\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,334 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,361 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,366 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,366 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,369 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,386 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,607 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[35mLog dir: /logs\u001b[0m\n",
      "\u001b[35mMetrics dir: /logs\u001b[0m\n",
      "\u001b[35mNetty threads: 0\u001b[0m\n",
      "\u001b[35mNetty client threads: 0\u001b[0m\n",
      "\u001b[35mDefault workers per model: 4\u001b[0m\n",
      "\u001b[35mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[35mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[35mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[35mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[35mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[35mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[35mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[35mEnable metrics API: true\u001b[0m\n",
      "\u001b[35mMetrics mode: log\u001b[0m\n",
      "\u001b[35mDisable system metrics: false\u001b[0m\n",
      "\u001b[35mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mModel config: N/A\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,334 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,361 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,366 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,366 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,369 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,386 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,607 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:18,619 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:18,619 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,131 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,285 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,287 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:47.633201599121094|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,288 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:8.231929779052734|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,289 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:14.7|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,290 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14030.3984375|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,291 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1204.890625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:19,293 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:9.8|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,131 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,285 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,287 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:47.633201599121094|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,288 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:8.231929779052734|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,289 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:14.7|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,290 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14030.3984375|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,291 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1204.890625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:19,293 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:9.8|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679279\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,003 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=58\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,004 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,022 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,023 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]58\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,024 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,025 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,029 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,040 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,045 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281045\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,090 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,109 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=53\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,110 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,124 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,125 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]53\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,126 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,126 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,128 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,129 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,130 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281130\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,003 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=58\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,004 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,022 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,023 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]58\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,024 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,025 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,029 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,040 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,045 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281045\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,090 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,109 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=53\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,110 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,124 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,125 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]53\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,126 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,126 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,128 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,129 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,130 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281130\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,300 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=56\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,305 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,335 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,339 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]56\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,342 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,343 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,343 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,343 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,345 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,347 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,347 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,348 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,350 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,351 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,351 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,352 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,352 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,353 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,354 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,354 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,355 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,355 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,300 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=56\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,305 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,335 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,339 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]56\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,342 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,343 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,343 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,343 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,345 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,347 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,347 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,348 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,350 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,351 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,351 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,352 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,352 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,353 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,354 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,354 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,355 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,355 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,355 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,356 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,356 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,357 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281357\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,356 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,366 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,371 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,420 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=55\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,421 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,426 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,427 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281427\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,427 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,427 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,428 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,449 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,457 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,458 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]55\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,461 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,461 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,461 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,484 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,484 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281484\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,355 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,356 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,356 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,357 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281357\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,356 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,366 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,367 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,371 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,420 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=55\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,421 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,426 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,427 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281427\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,427 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,427 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,428 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,449 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,457 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,458 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]55\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,461 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,461 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,461 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,484 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,484 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1707679281484\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,505 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,507 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,498 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,500 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,505 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,507 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,513 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,514 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,515 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281515\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,515 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,516 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,542 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,711 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,711 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,712 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,712 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,713 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,713 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,714 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,714 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,714 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,715 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,715 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,716 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,716 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,716 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,717 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,717 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,718 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,718 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,513 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,514 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,515 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281515\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,515 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,516 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,542 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,711 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,711 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,712 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,712 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,713 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,713 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,714 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,714 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,714 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,715 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,715 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,716 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,716 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,716 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,717 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,717 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,718 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,718 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,718 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,719 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,719 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,720 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,720 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,720 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,721 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,721 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,722 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,722 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,722 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,723 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,723 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,724 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,725 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,725 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,726 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281726\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,726 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,726 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,726 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,718 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,719 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,719 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,720 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,720 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,720 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,721 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,721 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,722 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,722 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,722 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,723 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,723 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,724 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,725 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,725 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,726 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281726\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,726 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,726 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,726 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 78, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     model = torch.jit.load(model_path, map_location=device)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,865 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_serialization.py\", line 162, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     cpp_module = torch._C.import_ir_module(cu, str(f), map_location, _extra_files, _restore_shapes)  # type: ignore[call-arg]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not found\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,867 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,867 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,867 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,869 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,870 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,871 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281871\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,871 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,871 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:21,872 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - The above exception was the direct cause of the following exception:\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - \u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 253, in <module>\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,866 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,867 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 221, in run_server\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,867 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,867 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 184, in handle_connection\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_service_worker.py\", line 131, in load_model\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/ts/model_loader.py\", line 135, in load\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,868 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir, context=context)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 184, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._model = self._run_handler_function(self._model_fn, *(model_dir,))\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,869 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_inference/transformer.py\", line 272, in _run_handler_function\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,869 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     result = func(*argv)\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py\", line 80, in default_model_fn\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     raise ModelLoadError(\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - sagemaker_pytorch_serving_container.default_pytorch_inference_handler.ModelLoadError: Failed to load /opt/ml/model/model.pt. Please ensure model is saved using torchscript.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,870 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,871 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1707679281871\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,871 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,871 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:21,872 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:22,086 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:38274 \"GET /ping HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:22,086 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679282\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:22,131 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:37972 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:22,132 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679282\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:22,286 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679282\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:22,086 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:38274 \"GET /ping HTTP/1.1\" 200 15\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:22,086 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679282\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:22,131 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:37972 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:22,132 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679282\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:22,286 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679282\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:22.142:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=10, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,855 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,855 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,856 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,857 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,857 [ERROR] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[34morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,855 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,855 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,856 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,857 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,857 [ERROR] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[35morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,887 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,888 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,888 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,888 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,892 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,892 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,896 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=124\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,897 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,915 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,915 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]124\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,916 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,916 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,917 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,921 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,922 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,922 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679284922\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,923 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:37976 \"POST /invocations HTTP/1.1\" 500 2639\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,923 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679284\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,924 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,887 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,888 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,888 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,888 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,892 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,892 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,896 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=124\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,897 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,915 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,915 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]124\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,916 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,916 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,917 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,921 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,922 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,922 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679284922\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,923 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:37976 \"POST /invocations HTTP/1.1\" 500 2639\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,923 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679284\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,924 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,927 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,933 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,934 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,944 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:24,944 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,927 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,933 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,934 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,944 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:24,944 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,040 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679285\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,094 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,095 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,096 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,094 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,096 [ERROR] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[34morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,040 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679285\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,094 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,095 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,096 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,094 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,096 [ERROR] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[35morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,098 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,099 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,101 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,102 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,113 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,114 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,115 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,115 [ERROR] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[34morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,116 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,116 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,117 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,114 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,117 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,120 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,121 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,127 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:25,127 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,098 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,099 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,101 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,102 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,113 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,114 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,115 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,115 [ERROR] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[35morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,116 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,116 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,117 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,114 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,117 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,120 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,121 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,127 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:25,127 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,162 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=161\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,163 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,180 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,180 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]161\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,180 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,162 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=161\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,163 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,180 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,180 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]161\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,180 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,182 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,181 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,184 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,185 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679288185\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,184 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,192 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker thread exception.\u001b[0m\n",
      "\u001b[34mio.netty.channel.StacklessClosedChannelException: null\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,193 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:37984 \"POST /invocations HTTP/1.1\" 500 3154\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,194 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,195 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,195 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,204 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,204 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,205 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=164\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,206 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,218 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,218 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]164\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,218 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,219 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,219 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,220 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,221 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,221 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,244 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,406 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,414 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,533 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=170\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,535 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,553 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,554 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]170\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,554 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,555 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,556 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,557 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,558 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,558 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,559 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,559 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,559 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,577 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=167\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,579 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,596 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]167\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,182 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,181 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,184 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,185 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679288185\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,184 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,192 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker thread exception.\u001b[0m\n",
      "\u001b[35mio.netty.channel.StacklessClosedChannelException: null\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,193 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:37984 \"POST /invocations HTTP/1.1\" 500 3154\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,194 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,195 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,195 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,204 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,204 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,205 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=164\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,206 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,218 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,218 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]164\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,218 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,219 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,219 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,220 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,221 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,221 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,244 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,406 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,414 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,533 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=170\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,535 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,553 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,554 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]170\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,554 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,555 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,556 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,557 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,558 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,558 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,559 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,559 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,559 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,577 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=167\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,579 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,596 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]167\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,598 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,599 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,599 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679288599\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,599 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker thread exception.\u001b[0m\n",
      "\u001b[34mio.netty.channel.StacklessClosedChannelException: null\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,600 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:37996 \"POST /invocations HTTP/1.1\" 500 357\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,600 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,601 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,601 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,602 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,602 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:28,626 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,597 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,598 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,599 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,599 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679288599\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,599 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker thread exception.\u001b[0m\n",
      "\u001b[35mio.netty.channel.StacklessClosedChannelException: null\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,600 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:37996 \"POST /invocations HTTP/1.1\" 500 357\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,600 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,601 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,601 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,602 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,602 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:28,626 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679288\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,714 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,714 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,729 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,730 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,741 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,741 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,745 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,745 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,745 [ERROR] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[34morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,746 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,747 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,748 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,748 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,755 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,755 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,862 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,865 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,865 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,866 [ERROR] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[34morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,714 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,714 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,729 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,730 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,741 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,741 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,745 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,745 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,745 [ERROR] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[35morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,746 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,747 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,748 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,748 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,755 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,755 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,862 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,865 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,865 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,866 [ERROR] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[35morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,867 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,867 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,867 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,868 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,869 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,873 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,874 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,875 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,875 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,875 [ERROR] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[34morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,876 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,876 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,876 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,876 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,873 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,873 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,881 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:31,881 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,867 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,867 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,867 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,868 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,869 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,873 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,874 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,875 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,875 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,875 [ERROR] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker error\u001b[0m\n",
      "\u001b[35morg.pytorch.serve.wlm.WorkerInitializationException: Backend stream closed.\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerLifeCycle.startWorker(WorkerLifeCycle.java:173) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.connect(WorkerThread.java:339) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:183) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:840) [?:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,876 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,876 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,876 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,876 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,873 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,873 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,881 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:31,881 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.194:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.194:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg: \u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.194:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg: Message:\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.195:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg: {\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.195:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg:   \"code\": 500,\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.195:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg:   \"type\": \"InternalServerException\",\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.195:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg:   \"message\": \"Worker died.\"\u001b[0m\n",
      "\u001b[32m2024-02-11T19:21:37.195:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000003.jpg: }\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,173 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:38008 \"POST /invocations HTTP/1.1\" 500 8548\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,174 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679297\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,174 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,175 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,183 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,185 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]242\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,202 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,202 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,203 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,203 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,203 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,221 [ERROR] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,227 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=245\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,228 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,246 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,246 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]245\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,246 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,247 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,248 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,173 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:38008 \"POST /invocations HTTP/1.1\" 500 8548\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,174 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679297\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,174 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,175 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,183 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,185 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]242\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,201 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,202 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,202 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,203 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,203 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,203 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,221 [ERROR] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,227 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=245\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,228 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,246 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,246 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]245\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,246 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,247 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,248 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,249 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,249 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,249 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,249 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,249 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,249 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:37,430 [INFO ] epollEventLoopGroup-3-7 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679297\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,249 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,249 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,249 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,249 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,249 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,249 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:37,430 [INFO ] epollEventLoopGroup-3-7 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679297\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:40,445 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:40,445 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:40,569 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:40,570 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:40,620 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:40,620 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:40,642 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:40,445 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:40,445 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:40,569 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:40,570 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:40,620 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:40,620 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:40,642 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=281\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,411 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,411 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]281\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,412 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,412 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,412 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,413 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,413 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679304413\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,414 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:35824 \"POST /invocations HTTP/1.1\" 500 6985\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,414 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,416 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679304\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,417 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,418 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,427 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,427 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,518 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=284\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,519 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,537 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]284\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,540 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,541 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,541 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,541 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,541 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,547 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=287\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,547 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]287\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,570 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,571 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,571 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,571 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,571 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,609 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,626 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=290\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,630 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=281\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,411 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,411 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]281\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,412 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,412 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,412 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,413 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,413 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679304413\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,414 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:35824 \"POST /invocations HTTP/1.1\" 500 6985\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,414 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,416 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679304\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,417 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,418 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,427 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,427 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,518 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=284\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,519 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,537 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]284\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,538 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,540 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,541 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,541 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,541 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,541 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,547 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=287\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,547 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]287\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,566 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,570 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,571 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,571 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,571 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,571 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,609 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,626 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=290\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,630 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]290\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,646 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,646 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,646 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,646 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,646 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:44,782 [INFO ] epollEventLoopGroup-3-8 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679304\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]290\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,645 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,646 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,646 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,646 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,646 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,646 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:44,782 [INFO ] epollEventLoopGroup-3-8 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679304\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,819 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,819 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,819 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,819 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,950 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,950 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,968 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,968 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,979 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:47,979 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,950 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,950 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,968 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,968 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,979 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:47,979 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,823 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,824 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,824 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679314824\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,828 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:34512 \"POST /invocations HTTP/1.1\" 500 10046\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,828 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679314\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,829 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,829 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,830 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,830 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,863 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=332\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,864 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,868 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=326\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,869 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]332\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,883 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,883 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,884 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,823 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,824 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,824 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679314824\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,828 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:34512 \"POST /invocations HTTP/1.1\" 500 10046\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,828 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679314\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,829 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,829 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,830 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,830 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,863 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=332\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,864 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,868 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=326\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,869 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]332\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,882 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,883 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,883 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,884 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,884 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,884 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,885 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]326\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,891 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,892 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,892 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,892 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,892 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:54,916 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679314\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,884 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,884 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,885 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]326\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,890 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,891 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,892 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,892 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,892 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,892 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:54,916 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679314\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,213 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=335\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,214 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]335\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,213 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=335\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,214 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,231 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]335\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,232 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,232 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,232 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,233 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679315233\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,233 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,234 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,234 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:55888 \"POST /invocations HTTP/1.1\" 500 319\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,235 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679315\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,235 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,236 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,246 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,247 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:55,259 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679315\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,232 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,232 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,232 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,233 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679315233\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,233 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,234 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,234 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:55888 \"POST /invocations HTTP/1.1\" 500 319\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,235 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679315\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,235 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,236 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,246 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,247 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:55,259 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679315\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,300 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,300 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,305 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,305 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,327 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,300 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,300 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,305 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,305 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,327 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,327 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,500 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:21:58,500 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,327 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,500 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:21:58,500 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg: \u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg: Message:\u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg: {\u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg:   \"code\": 500,\u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg:   \"type\": \"InternalServerException\",\u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg:   \"message\": \"Worker died.\"\u001b[0m\n",
      "\u001b[32m2024-02-11T19:22:10.268:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000002.jpg: }\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,099 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]376\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,099 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,099 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]376\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,099 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,099 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,100 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,101 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,102 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,102 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,102 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,102 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,102 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,189 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=370\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,190 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,207 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,208 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]370\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,208 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,211 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,209 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,219 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,219 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679330219\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,219 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:55892 \"POST /invocations HTTP/1.1\" 500 14960\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,220 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679330\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,222 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,220 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,223 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,223 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,223 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,245 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=373\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,245 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]373\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,265 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,267 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,267 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,267 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,267 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,267 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,270 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,279 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679330\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,691 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=379\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,692 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]379\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,711 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,711 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,711 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,711 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:10,711 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,099 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,100 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,101 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,102 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,102 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,102 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,102 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,102 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,189 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=370\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,190 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,207 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,208 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]370\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,208 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,211 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,209 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,219 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,219 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679330219\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,219 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:55892 \"POST /invocations HTTP/1.1\" 500 14960\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,220 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679330\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,222 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,220 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,223 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,223 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,223 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,245 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=373\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,245 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]373\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,264 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,265 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,267 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,267 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,267 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,267 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,267 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,270 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,279 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679330\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,691 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=379\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,692 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]379\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,710 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,711 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,711 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,711 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,711 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:10,711 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:13,509 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:13,647 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:13,668 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:13,953 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:13,953 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:13,509 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:13,647 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:13,668 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:13,953 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:13,953 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:19,155 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1525.22265625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:19,156 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:11.9|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:19,155 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1525.22265625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:19,156 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:11.9|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:19,155 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:13709.87890625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:19,155 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1525.22265625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:19,156 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:11.9|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:19,155 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:13709.87890625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:19,155 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1525.22265625|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:19,156 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:11.9|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679339\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,300 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=422\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,311 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,325 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,327 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]422\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,328 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,328 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,329 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,329 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,329 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,329 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,329 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,345 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,412 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=416\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,413 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,431 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]416\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,440 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,440 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,300 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9001, pid=422\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,311 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,325 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,327 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]422\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,328 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,328 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,329 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,329 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,329 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,329 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,329 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,345 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,412 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=416\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,413 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,431 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]416\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,439 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,440 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,440 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,440 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,440 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,440 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,456 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,645 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=419\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,645 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]419\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,666 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,666 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,666 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,666 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,666 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:33,666 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,440 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,440 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,440 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,456 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,645 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=419\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,645 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]419\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,664 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,666 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,666 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,666 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,666 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,666 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:33,666 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,040 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=425\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,040 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]425\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,060 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679354060\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:55802 \"POST /invocations HTTP/1.1\" 500 23782\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679354\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,040 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=425\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,040 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]425\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,059 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,060 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679354060\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:55802 \"POST /invocations HTTP/1.1\" 500 23782\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,060 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679354\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,060 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,061 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,061 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,061 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:34,075 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679354\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,060 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,061 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,061 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,061 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:34,075 [INFO ] epollEventLoopGroup-3-4 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679354\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:36,740 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:36,740 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:36,831 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:36,831 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:36,941 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:36,941 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:36,740 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:36,740 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:36,831 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:36,831 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:36,941 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:36,941 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:37,309 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:22:37,309 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:37,309 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:22:37,309 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,474 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,474 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]460\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,475 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,475 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,479 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,479 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,479 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,479 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,480 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,483 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,484 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,659 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=463\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,660 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,677 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,677 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]463\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,678 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,678 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,678 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,679 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,679 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,679 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,679 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,679 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,474 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,474 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]460\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,475 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,475 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,479 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,479 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,479 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,479 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,480 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,483 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,484 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,659 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=463\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,660 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,677 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,677 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]463\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,678 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,678 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,678 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,679 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,679 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,679 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,679 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,679 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:09,679 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:09,679 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 55 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,063 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=466\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,064 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,082 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,082 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]466\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,083 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,083 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,083 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,084 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,084 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679390084\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,085 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,085 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:39018 \"POST /invocations HTTP/1.1\" 500 36011\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,086 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,086 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,086 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,089 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,090 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,103 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,310 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=475\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,311 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,321 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,321 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]475\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,321 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,322 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,322 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,322 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,322 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679390322\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,323 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,323 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:55196 \"POST /invocations HTTP/1.1\" 500 220\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,323 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,323 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,323 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,331 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,331 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:10,338 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,063 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=466\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,064 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,082 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,082 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]466\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,083 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,083 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,083 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,084 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,084 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679390084\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,085 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,085 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:39018 \"POST /invocations HTTP/1.1\" 500 36011\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,086 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,086 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,086 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,089 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,090 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,103 [INFO ] epollEventLoopGroup-3-5 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,310 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=475\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,311 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,321 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,321 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]475\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,321 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,322 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,322 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,322 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,322 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679390322\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,323 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,323 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:55196 \"POST /invocations HTTP/1.1\" 500 220\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,323 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,323 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,323 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,331 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,331 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 55 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:10,338 [INFO ] epollEventLoopGroup-3-6 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679390\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:12,887 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:12,887 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:13,004 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:13,004 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:12,887 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:12,887 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:13,004 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:13,004 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:13,334 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:13,334 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:13,568 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:23:13,568 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:13,334 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:13,334 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:13,568 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:23:13,568 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg: \u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg: Message:\u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg: {\u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg:   \"code\": 500,\u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg:   \"type\": \"InternalServerException\",\u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg:   \"message\": \"Worker died.\"\u001b[0m\n",
      "\u001b[32m2024-02-11T19:24:06.830:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000001.jpg: }\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,487 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,487 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,487 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,487 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,488 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,489 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,490 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,492 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,498 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,499 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,491 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,499 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,800 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=507\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,801 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,819 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]507\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,821 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,821 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679446821\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,821 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,822 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker thread exception.\u001b[0m\n",
      "\u001b[34mio.netty.channel.StacklessClosedChannelException: null\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,823 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:55200 \"POST /invocations HTTP/1.1\" 500 56485\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,824 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679446\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,824 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,825 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,825 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,488 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,489 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,490 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,492 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,498 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,499 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,491 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,499 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,800 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=507\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,801 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,819 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]507\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,820 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,821 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,821 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679446821\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,821 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,822 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker thread exception.\u001b[0m\n",
      "\u001b[35mio.netty.channel.StacklessClosedChannelException: null\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,823 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:55200 \"POST /invocations HTTP/1.1\" 500 56485\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,824 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679446\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,824 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,825 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,825 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,826 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:06,868 [INFO ] epollEventLoopGroup-3-7 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679446\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,826 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 89 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:06,868 [INFO ] epollEventLoopGroup-3-7 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679446\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,243 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=513\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,244 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]513\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,263 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,264 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,264 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,264 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,264 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,265 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,265 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,467 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=519\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,467 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,477 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]519\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,479 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,479 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679447479\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,479 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,480 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:35944 \"POST /invocations HTTP/1.1\" 500 613\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,480 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679447\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,481 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,481 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,481 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,481 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:07,491 [INFO ] epollEventLoopGroup-3-8 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679447\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,243 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=513\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,244 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]513\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,262 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,263 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,264 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,264 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,264 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,264 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,265 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,265 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,467 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=519\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,467 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,477 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]519\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,478 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,479 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,479 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679447479\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,479 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,480 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:35944 \"POST /invocations HTTP/1.1\" 500 613\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,480 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679447\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,481 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,481 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,481 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,481 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 89 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:07,491 [INFO ] epollEventLoopGroup-3-8 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679447\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:09,944 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:09,944 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:10,220 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:10,220 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:10,504 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:10,504 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:10,725 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:24:10,725 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:10,220 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:10,220 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:10,504 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:10,504 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:10,725 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:24:10,725 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,364 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,364 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,364 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,364 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,364 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,364 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,365 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,365 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,365 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:37,380 [ERROR] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,039 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=554\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,039 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]554\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,062 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,063 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,063 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,063 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,063 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,063 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,077 [ERROR] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,364 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,364 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,365 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,365 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,365 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:37,380 [ERROR] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,039 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=554\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,039 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]554\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,061 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,062 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,063 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,063 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,063 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,063 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,063 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 144 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,077 [ERROR] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,346 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=562\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,346 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]562\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,363 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,364 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,364 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,364 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,364 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,365 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,377 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,606 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=570\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,606 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]570\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,626 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,626 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,626 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,627 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,627 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:38,627 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,346 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=562\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,346 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]562\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,362 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,363 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,364 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,364 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,364 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,364 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,365 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,377 [ERROR] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,606 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=570\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,606 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]570\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,625 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,626 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,626 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,626 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,627 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,627 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:38,627 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 144 seconds.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:40,788 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:40,788 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:41,413 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:41,413 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:41,612 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:41,612 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:41,872 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:25:41,872 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:41,413 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:41,413 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:41,612 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:41,612 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:41,872 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:25:41,872 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:02,859 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:02,863 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:02,863 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:02,863 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:02,864 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:02,864 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:02,859 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:02,863 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:02,863 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:02,863 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:02,864 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:02,864 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,109 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=609\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,110 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,109 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9002, pid=609\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,110 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,129 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,130 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]609\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,131 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,131 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,133 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,133 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,133 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,134 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,146 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=614\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,447 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,465 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,465 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]614\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,465 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,466 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,466 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,467 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,468 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679684467\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,468 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,468 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:35958 \"POST /invocations HTTP/1.1\" 500 236977\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,469 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,469 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,469 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,470 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,480 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,537 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=622\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,537 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,556 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,556 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]622\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,556 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,557 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,557 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,559 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,558 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679684558\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,557 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,559 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:43006 \"POST /invocations HTTP/1.1\" 500 79\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,560 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,129 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,130 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]609\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,131 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,131 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,133 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,133 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,133 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,134 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,146 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=614\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,447 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,465 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,465 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]614\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,465 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,466 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,466 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,467 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,468 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679684467\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,468 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,468 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:35958 \"POST /invocations HTTP/1.1\" 500 236977\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,469 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,469 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,469 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,470 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,480 [INFO ] epollEventLoopGroup-3-1 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,537 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9003, pid=622\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,537 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,556 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,556 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]622\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,556 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,557 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,557 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,559 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,558 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1707679684558\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,557 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,559 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:43006 \"POST /invocations HTTP/1.1\" 500 79\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,560 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1.0|#Level:Host|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,560 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,560 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,560 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:04,580 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,560 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,560 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,560 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:04,580 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707679684\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:06,241 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:06,241 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:06,241 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:06,241 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:07,485 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:07,485 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:07,485 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:07,722 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:07,722 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:07,805 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2024-02-11T19:28:07,805 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:07,485 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:07,722 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:07,722 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:07,805 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2024-02-11T19:28:07,805 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[32m2024-02-11T19:38:04.676:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000004.jpg: Model server did not respond to /invocations request within 600 seconds\u001b[0m\n",
      "\u001b[32m2024-02-11T20:18:05.071:[sagemaker logs]: sagemaker-us-east-1-752648173624/safety/data/split/batch/images/000006.jpg: Model server did not respond to /invocations request within 600 seconds\u001b[0m\n",
      "\u001b[34m2024-02-11T20:28:05,163 [INFO ] epollEventLoopGroup-3-8 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707683285\u001b[0m\n",
      "\u001b[35m2024-02-11T20:28:05,163 [INFO ] epollEventLoopGroup-3-8 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:76a5cd3af081,timestamp:1707683285\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:28\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/transformer.py:318\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, batch_data_capture_config, wait, logs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_transform_job \u001b[38;5;241m=\u001b[39m _TransformJob\u001b[38;5;241m.\u001b[39mstart_new(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    304\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     batch_data_capture_config,\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_transform_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/transformer.py:685\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m--> 685\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_transform_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_transform_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sagemaker/session.py:5702\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   5699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE:\n\u001b[1;32m   5700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 5702\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   5705\u001b[0m     state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.pytorch.model import PyTorchModel # PyTorch model\n",
    "\n",
    "# define paths\n",
    "model_data = f\"{output_location}/{job_name}/output/model.tar.gz\" # trained model artifacts\n",
    "transformer_input = f\"{s3_split_dest}batch/images\" # batch directory for input data\n",
    "transformer_output = f\"{output_location}/{job_name}/output/transformer\" # transformer job results\n",
    "\n",
    "# create a PyTorch model\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_data, # trained model artifacts\n",
    "    role=role,\n",
    "    entry_point='inference.py', # custom inference script, locate in code directory\n",
    "    framework_version='2.1.0', # CPU - Python 3.10\n",
    "    py_version='py310',\n",
    "    source_dir='code',\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# create a transformer from the PyTorch model\n",
    "transformer = pytorch_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=transformer_output,\n",
    "    accept='application/json',\n",
    "    max_payload=10\n",
    ")\n",
    "\n",
    "# begin batch transform job\n",
    "transformer.transform(\n",
    "    data=transformer_input,\n",
    "    content_type='image/jpeg'\n",
    ")\n",
    "\n",
    "# wait for job to complete\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad8656-c7fa-40a3-bcae-fe854e846ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define s3 client\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# define prefix for transformer job output\n",
    "transformer_output_s3_prefix = transformer_output.replace(\n",
    "    f\"s3://{bucket}/\", \"\"\n",
    ")\n",
    "\n",
    "# get job output files from bucket\n",
    "objects_in_bucket = s3_client.list_objects(\n",
    "    Bucket=bucket, Prefix=transformer_output_s3_prefix\n",
    ")\n",
    "\n",
    "# define function to convert an s3 file to dataframe\n",
    "def s3_file_to_df(key):\n",
    "    \n",
    "    # get single s3 file\n",
    "    s3_file = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    \n",
    "    # decode, read, and create list of json strings\n",
    "    body = s3_file['Body'].read().decode('utf-8')\n",
    "    json_strings = json.loads(body)\n",
    "    \n",
    "    # create list for data to create dataframe\n",
    "    data_list = []\n",
    "\n",
    "    # parse json strings and append to data list\n",
    "    for json_str in json_strings:\n",
    "        json_obj = json.loads(json_str)\n",
    "        data_list.append(json_obj)\n",
    "        \n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # insert column for sample id, obtained from s3 filename\n",
    "    df.insert(0, 'sample_id', key.split(\"/\")[-1].split(\".\")[0])\n",
    "    return df\n",
    "\n",
    "# create list to store dataframes for concatenation\n",
    "df_list = []\n",
    "\n",
    "# iterate through each object in s3 bucket\n",
    "for obj in objects_in_bucket.get('Contents', []):\n",
    "    \n",
    "    # get filename\n",
    "    s3_filename = obj['Key']\n",
    "\n",
    "    # skip over any files without expected '.out' file extension\n",
    "    if s3_filename.endswith('.out'):\n",
    "\n",
    "        # call function to create dataframe of current file contents\n",
    "        df_current_file = s3_file_to_df(s3_filename)\n",
    "        \n",
    "        # insert column for timestamp\n",
    "        df_current_file.insert(0, 'timestamp', obj['LastModified'])\n",
    "\n",
    "        # append to dataframe list for concatenation\n",
    "        df_list.append(df_current_file)\n",
    "\n",
    "# concatenate dataframes to create end result\n",
    "df_batch_transform = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# convert class id to int\n",
    "df_batch_transform['class_id'] = df_batch_transform['class_id'].astype(int)\n",
    "\n",
    "# display dataframe\n",
    "df_batch_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77faa367-d273-45d1-afde-57a498308ccc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example Business Problem Query\n",
    "\n",
    "Query the dataframe to obtain number of detections of each class we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e1395-e5f7-4015-af82-ae237620233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe copy\n",
    "df_business_query = df_batch_transform.copy()\n",
    "\n",
    "# add a date column in YYYY-MM-DD format derived from timestamp\n",
    "df_business_query['date'] = df_batch_transform['timestamp'].dt.date\n",
    "\n",
    "# create new dataframe with business query results\n",
    "df_business_query = pd.DataFrame(df_business_query.groupby(['date', 'class_id', 'class_name']).size().reset_index(name='detections'))\n",
    "\n",
    "# filter out class id of 1 for 'vest'\n",
    "df_business_query = df_business_query.query('class_id != 1')\n",
    "\n",
    "# display dataframe\n",
    "df_business_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c430b-eaae-4dd7-b143-97d81c362b1a",
   "metadata": {},
   "source": [
    "# Monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee937572-2dbf-4cc4-bb1f-3a7123b331dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker import get_execution_role, session, Session\n",
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker.model_monitor import CronExpressionGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d46ade-de6e-4c37-9425-28420c387734",
   "metadata": {},
   "source": [
    "### Create Baseline\n",
    "Create a baseline job that compares your model predictions with ground truth labels in a baseline dataset that you have stored in Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66caf8dd-a8b4-4bfe-90a1-120f03f6799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_uri = f\"{s3_split_dest}batch/images\" # validation data\n",
    "baseline_results_uri = f\"{output_location}/{job_name}/output/transformer\" # predicted data\n",
    "print(f\"Baseline data uri: {baseline_data_uri}\")\n",
    "print(f\"Baseline results uri: {baseline_results_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f42143-3e5c-4f5b-bd27-ced1795fb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combination of validatioon and predicted data\n",
    "baseline_dataset_uri = S3Uploader.upload(f\"test_data/{validate_dataset}\", baseline_data_uri)\n",
    "baseline_dataset_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f66cb-0ef6-48a2-b00e-ee18ae2c088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job_name = \"MyBaseLineJob\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dc981-9b34-4721-8d28-e2d10e458f1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "session = Session()\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=sagemaker.get_execution_role(), # role\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Execute the baseline suggestion job.\n",
    "baseline_job_name = \"MyBaseLineJob\"\n",
    "job = model_quality_monitor.suggest_baseline(\n",
    "    job_name=baseline_job_name,\n",
    "    baseline_dataset=baseline_dataset_uri, # The S3 location of the validation dataset.\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri = baseline_results_uri, # The S3 location to store the results.\n",
    "    problem_type='Classification',\n",
    "    inference_attribute= \"class_name\", # The column in the dataset that contains predictions.\n",
    "    probability_attribute= \"confidence\", # The column in the dataset that contains probabilities.\n",
    "    ground_truth_attribute= \"??????\" # The column in the dataset that contains ground truth labels.\n",
    ")\n",
    "job.wait(logs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ec10f-b9ea-4524-98c1-dfd393f49dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraints that the job generated\n",
    "baseline_job = model_quality_monitor.latest_baselining_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4eca2-8427-48ae-9e83-58138b1aa354",
   "metadata": {},
   "source": [
    "### Implement model monitors on your ML system\n",
    "Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2604e-d337-4e5b-884a-9f6cb5a63749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quality_model_monitor = ModelQualityMonitor(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    ")\n",
    "\n",
    "schedule = model_quality_model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    batch_transform_input=BatchTransformInput(\n",
    "        data_captured_destination_s3_uri=s3_capture_upload_path,\n",
    "        destination=\"/opt/ml/processing/input\", \n",
    "        dataset_format=MonitoringDatasetFormat.csv(header=False),\n",
    "        # the column index of the output representing the inference probablity\n",
    "        probability_attribute=\"0\",\n",
    "        # the threshold to classify the inference probablity to class 0 or 1 in \n",
    "        # binary classification problem\n",
    "        probability_threshold_attribute=0.5,\n",
    "        # look back 6 hour for transform job outputs.\n",
    "        start_time_offset=\"-PT6H\",\n",
    "        end_time_offset=\"-PT0H\"\n",
    "    ),\n",
    "    ground_truth_input=gt_s3_uri,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    problem_type=\"Classification\",\n",
    "    constraints = baseline_job.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a83de8-a569-45e1-a79a-95f19eeff868",
   "metadata": {},
   "source": [
    "### Implement data monitors on your ML system.\n",
    "Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24100237-ea6a-4636-bc54-2e9a6c3af5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_model_monitor = DefaultModelMonitor(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    ")\n",
    "\n",
    "schedule = data_quality_model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    batch_transform_input=BatchTransformInput(\n",
    "        data_captured_destination_s3_uri=s3_capture_upload_path,\n",
    "        destination=\"?????\", #/opt/ml/processing/input\n",
    "        dataset_format=MonitoringDatasetFormat.csv(header=False),\n",
    "    ),\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics= statistics_path,\n",
    "    constraints = baseline_job.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe612a-7ddf-471e-b8e3-8b0963a802bf",
   "metadata": {},
   "source": [
    "### Implement infrastructure monitors on your ML system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543d463-80b1-446a-b0fe-8c436cc91cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2856654b-70fe-4a40-8cc7-6797abdd27a2",
   "metadata": {},
   "source": [
    "### Create a monitoring dashboard for your ML endpoint/job on CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b85c19-dfb0-49e7-bd24-3201dbcac1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CloudWatch client\n",
    "cw_client = boto3.Session().client(\"cloudwatch\")\n",
    "\n",
    "namespace = \"aws/sagemaker/Endpoints/model-metrics\"\n",
    "\n",
    "cw_dimensions = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f27c3-4698-4c46-8841-4b731e7a3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Alarm\n",
    "alarm_name = \"MODEL_QUALITY_F2_SCORE\"\n",
    "alarm_desc = (\n",
    "    \"Trigger an CloudWatch alarm when the f2 score drifts away from the baseline constraints\"\n",
    ")\n",
    "mdoel_quality_f2_drift_threshold = (\n",
    "    0.625  ##Setting this threshold purposefully low to see the alarm quickly.\n",
    ")\n",
    "metric_name = \"f2\"\n",
    "namespace = \"?????\"\n",
    "\n",
    "cw_client.put_metric_alarm(\n",
    "    AlarmName=alarm_name,\n",
    "    AlarmDescription=alarm_desc,\n",
    "    ActionsEnabled=True,\n",
    "    MetricName=metric_name,\n",
    "    Namespace=namespace,\n",
    "    Statistic=\"Average\",\n",
    "    Dimensions=[\n",
    "\n",
    "    ],\n",
    "    Period=600,\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Threshold=mdoel_quality_f2_drift_threshold,\n",
    "    ComparisonOperator=\"LessThanOrEqualToThreshold\",\n",
    "    TreatMissingData=\"breaching\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f50686-9823-4f2e-8db1-523f407929d1",
   "metadata": {},
   "source": [
    "### Generate model and data reports on SageMaker.\n",
    "\n",
    "from the frontend"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
